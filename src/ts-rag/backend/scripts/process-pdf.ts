/**
 * Script para processar o PDF e gerar embeddings
 *
 * Etapas:
 * 1. Extrair texto do PDF
 * 2. Dividir em chunks (RecursiveCharacterTextSplitter)
 * 3. Classificar metadata de cada chunk
 * 4. Gerar embeddings com Gemini (respeitando rate limit)
 * 5. Salvar no MongoDB Atlas
 */

import { GoogleGenerativeAIEmbeddings } from '@langchain/google-genai'
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import dotenv from 'dotenv'
import { MongoClient } from 'mongodb'
import fs from 'node:fs'
import path from 'node:path'
import { fileURLToPath } from 'node:url'
import pdfParse from 'pdf-parse'
import { processChunkMetadata, resetContext } from '../src/shared/utils/chunk-classifier.js'
import { RateLimiter } from '../src/shared/utils/rate-limiter.js'

// Load environment variables
dotenv.config()

// Resolve paths
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const PDF_PATH = path.resolve(__dirname, '../../data/Essential-typescript-5-third-edition.pdf')

const MONGODB_URI = process.env.MONGODB_URI!
const DATABASE_NAME = process.env.DATABASE_NAME || 'ts_rag'
const GEMINI_API_KEY = process.env.GEMINI_API_KEY!

/**
 * Extrai texto do PDF
 */
async function extractPDFText(pdfPath: string): Promise<string> {
  console.log(`\nüìÑ Extraindo texto do PDF: ${path.basename(pdfPath)}`)

  const dataBuffer = fs.readFileSync(pdfPath)
  const data = await pdfParse(dataBuffer)

  console.log(`‚úÖ PDF extra√≠do com sucesso!`)
  console.log(`   - P√°ginas: ${data.numpages}`)
  console.log(`   - Caracteres: ${data.text.length.toLocaleString()}`)

  return data.text
}

/**
 * Divide texto em chunks com metadata
 */
async function createChunks(text: string) {
  console.log(`\n‚úÇÔ∏è  Dividindo texto em chunks...`)

  // RecursiveCharacterTextSplitter: melhor op√ß√£o para texto longo
  // - Tenta manter par√°grafos juntos
  // - Fallback para senten√ßas, depois palavras
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000, // ~200-300 tokens (1 char ~= 0.25 tokens)
    chunkOverlap: 200, // Overlap para manter contexto entre chunks
    separators: ['\n\n', '\n', '. ', ' ', ''], // Ordem de prefer√™ncia
  })

  const docs = await splitter.createDocuments([text])

  console.log(`‚úÖ ${docs.length} chunks criados`)
  console.log(`   - Tamanho m√©dio: ${Math.round(text.length / docs.length)} chars`)

  // Reseta contexto antes de processar (importante para context propagation)
  console.log(`\nüîÑ Resetando contexto do Chunk Classifier V2...`)
  resetContext()

  // Adiciona metadata a cada chunk
  const chunksWithMetadata = docs.map((doc, index) => {
    const metadata = processChunkMetadata(doc.pageContent, index, docs.length)

    return {
      text: doc.pageContent,
      metadata: {
        ...metadata,
        bookTitle: 'Essential TypeScript 5',
      },
    }
  })

  // Log de estat√≠sticas por tipo
  const typeStats = chunksWithMetadata.reduce((acc, chunk) => {
    const type = chunk.metadata.type
    acc[type] = (acc[type] || 0) + 1
    return acc
  }, {} as Record<string, number>)

  console.log(`\nüìä Distribui√ß√£o por tipo:`)
  Object.entries(typeStats).forEach(([type, count]) => {
    const percentage = ((count / chunksWithMetadata.length) * 100).toFixed(1)
    console.log(`   - ${type}: ${count} (${percentage}%)`)
  })

  return chunksWithMetadata
}

/**
 * Gera embeddings e salva no MongoDB
 */
async function processEmbeddings(chunks: Array<{
  text: string
  metadata: any
}>) {
  console.log(`\nüîå Conectando ao MongoDB Atlas...`)

  const client = new MongoClient(MONGODB_URI)
  await client.connect()

  const db = client.db(DATABASE_NAME)
  const collection = db.collection('embeddings')

  console.log(`‚úÖ Conectado!`)

  // Limpa collection antes de processar (opcional)
  const existingCount = await collection.countDocuments()
  if (existingCount > 0) {
    console.log(`\n‚ö†Ô∏è  Collection j√° cont√©m ${existingCount} documentos`)
    console.log(`   Limpando collection...`)
    await collection.deleteMany({})
    console.log(`‚úÖ Collection limpa`)
  }

  // Inicializa embeddings model
  console.log(`\nü§ñ Inicializando Gemini Embeddings...`)

  const embeddingModel = process.env.GEMINI_EMBEDDING_MODEL || 'gemini-embedding-001'

  const embeddings = new GoogleGenerativeAIEmbeddings({
    apiKey: GEMINI_API_KEY,
    model: embeddingModel,
  })

  console.log(`‚úÖ Gemini Embeddings pronto! (${embeddingModel})`)

  // Rate limiter (90 RPM com margem de seguran√ßa)
  const rateLimiter = new RateLimiter(90, 60000)

  console.log(`\n‚ö° Processando ${chunks.length} chunks...`)
  console.log(`   Rate limit: 90 req/min (FREE tier: 100 RPM)`)
  console.log(`   Tempo estimado: ~${Math.ceil(chunks.length / 90)} minutos\n`)

  let processed = 0
  const startTime = Date.now()

  for (const chunk of chunks) {
    // Aguarda rate limit
    await rateLimiter.wait()

    try {
      // Gera embedding
      const embedding = await embeddings.embedQuery(chunk.text)

      // Insere no MongoDB
      await collection.insertOne({
        text: chunk.text,
        embedding,
        metadata: chunk.metadata,
        createdAt: new Date(),
      })

      processed++

      // Log de progresso a cada 10 chunks
      if (processed % 10 === 0 || processed === chunks.length) {
        const elapsed = Math.round((Date.now() - startTime) / 1000)
        const rate = (processed / elapsed) * 60
        const remaining = chunks.length - processed
        const eta = remaining > 0 ? Math.round(remaining / rate) : 0

        const stats = rateLimiter.getStats()

        console.log(
          `üìä [${processed}/${chunks.length}] ` +
          `${((processed / chunks.length) * 100).toFixed(1)}% | ` +
          `Taxa: ${rate.toFixed(1)} req/min | ` +
          `ETA: ${eta}min | ` +
          `Restantes: ${stats.remainingRequests} req`
        )
      }
    } catch (error) {
      console.error(`‚ùå Erro ao processar chunk ${processed + 1}:`, error)
      throw error
    }
  }

  const totalTime = Math.round((Date.now() - startTime) / 1000)

  console.log(`\n‚úÖ Processamento conclu√≠do!`)
  console.log(`   - Chunks processados: ${processed}`)
  console.log(`   - Tempo total: ${Math.floor(totalTime / 60)}min ${totalTime % 60}s`)
  console.log(`   - Taxa m√©dia: ${((processed / totalTime) * 60).toFixed(1)} req/min`)

  await client.close()
  console.log(`\nüîå Conex√£o fechada`)
}

/**
 * Main
 */
async function main() {
  try {
    console.log(`\n${'='.repeat(60)}`)
    console.log(`üöÄ TypeScript RAG - Processamento do PDF`)
    console.log(`${'='.repeat(60)}`)

    // 1. Extrai PDF
    const text = await extractPDFText(PDF_PATH)

    // 2. Cria chunks com metadata
    const chunks = await createChunks(text)

    // 3. Gera embeddings e salva
    await processEmbeddings(chunks)

    console.log(`\n${'='.repeat(60)}`)
    console.log(`‚úÖ Processo conclu√≠do com sucesso!`)
    console.log(`${'='.repeat(60)}\n`)

    process.exit(0)
  } catch (error) {
    console.error(`\n‚ùå Erro durante o processamento:`, error)
    process.exit(1)
  }
}

main()
